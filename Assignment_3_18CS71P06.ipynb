{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Roll No : 18CS71P06\n",
    "#### Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sukanya/anaconda3/envs/mxnet/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import sys, os\n",
    "from mxnet.contrib import text\n",
    "filename = sys.argv[0]\n",
    "os.path.abspath(filename+\"/..\")\n",
    "np.random.seed(42)\n",
    "mx.random.seed(42)\n",
    "'''\n",
    "First job is to clean and preprocess the social media text. (5)\n",
    "\n",
    "1) Replace URLs and mentions (i.e strings which are preceeded with @)\n",
    "2) Segment #hastags \n",
    "3) Remove emoticons and other unicode characters\n",
    "'''\n",
    "\n",
    "def preprocess_tweet(input_text):\n",
    "    cleaned_text= input_text.lower()\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]','',cleaned_text)\n",
    "    cleaned_text = re.sub(r\"http\\S+\", \"\", cleaned_text)\n",
    "    cleaned_text = re.sub(r\"@\\S+\", \"\", cleaned_text)\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                u\"\\U00002702-\\U000027B0\"\n",
    "                u\"\\U000024C2-\\U0001F251\"\n",
    "                u\"\\U0001f926-\\U0001f937\"\n",
    "                u'\\U00010000-\\U0010ffff'\n",
    "                u\"\\u200d\"\n",
    "                u\"\\u2640-\\u2642\"\n",
    "                u\"\\u2600-\\u2B55\"\n",
    "                u\"\\u23cf\"\n",
    "                u\"\\u23e9\"\n",
    "                u\"\\u231a\"\n",
    "                u\"\\u3030\"\n",
    "                u\"\\ufe0f\"\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "    cleaned_text = emoji_pattern.sub(r'', cleaned_text)\n",
    "    cleaned_text = cleaned_text.replace(\"#\", \"\")\n",
    "    cleaned_text = re.sub(\"([a-z])([A-Z])\",\"\\g<1> \\g<2>\",cleaned_text)\n",
    "        #print(input_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('cancer_data.tsv')\n",
    "pos_data=[]\n",
    "neg_data=[]\n",
    "\n",
    "for line in file:\n",
    "    line=line.strip().split('\\t')\n",
    "    #print(line[0])\n",
    "    text2= preprocess_tweet(line[0]).strip().split()\n",
    "    if line[1]=='yes':\n",
    "        pos_data.append(text2)\n",
    "    if line[1]=='no':\n",
    "        neg_data.append(text2)\n",
    "\n",
    "#print(len(pos_data), len(neg_data))\n",
    "sentences= list(pos_data)\n",
    "sentences.extend(neg_data)\n",
    "pos_labels= [1 for _ in pos_data]\n",
    "neg_labels= [0 for _ in neg_data]\n",
    "y=list(pos_labels)\n",
    "y.extend(neg_labels)\n",
    "y=np.array(y)\n",
    "\n",
    "    #if line[1]=='yes':\n",
    "    #    pos_data.append(text2)\n",
    "    #if line[1]=='no':\n",
    "     #   neg_data.append(text2)\n",
    "\n",
    "#print(len(pos_data), len(neg_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_vectors(sentences):\n",
    "    '''\n",
    "    Input: List of sentences\n",
    "    Output: List of word vectors corresponding to each sentence, vocabulary\n",
    "    '''\n",
    "    padding_word = \"\" \n",
    "    sequence_length = max(len(x) for x in sentences)\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "    #print(padded_sentences[0])    \n",
    "    seen, result = set(), []\n",
    "    for item in padded_sentences:\n",
    "        for word in item:\n",
    "            #if word not in seen:\n",
    "            seen.add(word)\n",
    "            result.append(word)\n",
    "    #print(result)\n",
    "    wordfreq = {}\n",
    "    d={}\n",
    "    for word in result:\n",
    "        if word not in wordfreq:\n",
    "             wordfreq[word] = 0 \n",
    "        wordfreq[word] += 1\n",
    "    #l = wordfreq.items()\n",
    "    d = sorted(wordfreq.items(), key=lambda x:x[1], reverse=True)\n",
    "    #l.sort(key = lambda item: item[1])\n",
    "    #wordfreq = [result.count(p) for p in result]\n",
    "    #print(dict(zip(result,wordfreq)))\n",
    "    #keylist = wordfreq.keys()\n",
    "    #keylist.sort(reverse=true)\n",
    "    vocabulary_inv = [x[0] for x in d]\n",
    "    #print(d)\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    word_vectors = np.array([\n",
    "            [vocabulary[word] for word in sentence]\n",
    "            for sentence in padded_sentences])\n",
    "    return word_vectors, vocabulary, padded_sentences\n",
    "#create_word_vectors(sentences)\n",
    "\n",
    "x, vocabulary, padded_sentences = create_word_vectors(sentences)\n",
    "vocab_size=len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shuffle(x,y):\n",
    "    '''\n",
    "    Create an equal distribution of the positive and negative examples. \n",
    "    Please do not change this particular shuffling method.\n",
    "    '''\n",
    "    pos_len= len(pos_data)\n",
    "    neg_len= len(neg_data)\n",
    "    pos_len_train= int(0.8*pos_len)\n",
    "    neg_len_train= int(0.8*neg_len)\n",
    "    train_data= [(x[i],y[i]) for i in range(0, pos_len_train)]\n",
    "    train_data.extend([(x[i],y[i]) for i in range(pos_len, pos_len+ neg_len_train )])\n",
    "    test_data=[(x[i],y[i]) for i in range(pos_len_train, pos_len)]\n",
    "    test_data.extend([(x[i],y[i]) for i in range(pos_len+ neg_len_train, len(x) )])\n",
    "    \n",
    "    np.random.shuffle(train_data)\n",
    "    x_train=[i[0] for i in train_data]\n",
    "    y_train=[i[1] for i in train_data]\n",
    "    np.random.shuffle(test_data)\n",
    "    x_test=[i[0] for i in test_data]\n",
    "    y_test=[i[1] for i in test_data]\n",
    "    \n",
    "    x_train=np.array(x_train)\n",
    "    y_train=np.array(y_train)\n",
    "    x_test= np.array(x_test)\n",
    "    y_test= np.array(y_test)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test= create_shuffle(x,y)\n",
    "sentence_size = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size 20\n",
      "embedding dimensions 200\n",
      "convolution filters [2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "print('batch size', batch_size)\n",
    "\n",
    "input_x = mx.sym.Variable('data') # placeholder for input data\n",
    "input_y = mx.sym.Variable('softmax_label') # placeholder for output label\n",
    "num_embed = 200 # dimensions to embed words into\n",
    "print('embedding dimensions', num_embed)\n",
    "\n",
    "embed_layer = mx.sym.Embedding(data=input_x, input_dim=vocab_size, output_dim=num_embed, name='vocab_embed')\n",
    "\n",
    "# reshape embedded data for next layer\n",
    "conv_input = mx.sym.Reshape(data=embed_layer, shape=(batch_size, 1, sentence_size, num_embed))\n",
    "filter_list=[2,3, 4, 5] # the size of filters to use\n",
    "print('convolution filters', filter_list)\n",
    "\n",
    "num_filter=100\n",
    "pooled_outputs = []\n",
    "for filter_size in filter_list:\n",
    "    convi = mx.sym.Convolution(data=conv_input, kernel=(filter_size, num_embed), num_filter=num_filter)\n",
    "    relui = mx.sym.Activation(data=convi, act_type='relu')\n",
    "    pooli = mx.sym.Pooling(data=relui, pool_type='max', kernel=(sentence_size - filter_size + 1, 1), stride=(1, 1))\n",
    "    pooled_outputs.append(pooli)\n",
    "\n",
    "# combine all pooled outputs\n",
    "total_filters = num_filter * len(filter_list)\n",
    "concat = mx.sym.Concat(*pooled_outputs, dim=1)\n",
    "\n",
    "# reshape for next layer\n",
    "h_pool = mx.sym.Reshape(data=concat, shape=(batch_size, total_filters))\n",
    "\n",
    "num_label = 2\n",
    "\n",
    "cls_weight = mx.sym.Variable('cls_weight')\n",
    "cls_bias = mx.sym.Variable('cls_bias')\n",
    "\n",
    "fc = mx.sym.FullyConnected(data=h_pool, weight=cls_weight, bias=cls_bias, num_hidden=num_label)\n",
    "\n",
    "# softmax output\n",
    "sm = mx.sym.SoftmaxOutput(data=fc, label=input_y, name='softmax')\n",
    "\n",
    "# set CNN pointer to the \"back\" of the network\n",
    "cnn = sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Define the structure of our CNN Model (as a named tuple)\n",
    "CNNModel = namedtuple(\"CNNModel\", ['cnn_exec', 'symbol', 'data', 'label', 'param_blocks'])\n",
    "\n",
    "# Define what device to train/test on, use GPU if available\n",
    "ctx = mx.cpu()\n",
    "arg_names = cnn.list_arguments()\n",
    "\n",
    "input_shapes = {}\n",
    "input_shapes['data'] = (batch_size, sentence_size)\n",
    "\n",
    "arg_shape, out_shape, aux_shape = cnn.infer_shape(**input_shapes)\n",
    "arg_arrays = [mx.nd.zeros(s, ctx) for s in arg_shape]\n",
    "args_grad = {}\n",
    "for shape, name in zip(arg_shape, arg_names):\n",
    "    if name in ['softmax_label', 'data']: # input, output\n",
    "        continue\n",
    "    args_grad[name] = mx.nd.zeros(shape, ctx)\n",
    "\n",
    "cnn_exec = cnn.bind(ctx=ctx, args=arg_arrays, args_grad=args_grad, grad_req='add')\n",
    "\n",
    "param_blocks = []\n",
    "arg_dict = dict(zip(arg_names, cnn_exec.arg_arrays))\n",
    "initializer = mx.initializer.Uniform(0.1)\n",
    "for i, name in enumerate(arg_names):\n",
    "    if name in ['softmax_label', 'data']: # input, output\n",
    "        continue\n",
    "    initializer(mx.init.InitDesc(name), arg_dict[name])\n",
    "\n",
    "    param_blocks.append( (i, arg_dict[name], args_grad[name], name) )\n",
    "\n",
    "data = cnn_exec.arg_dict['data']\n",
    "label = cnn_exec.arg_dict['softmax_label']\n",
    "\n",
    "cnn_model= CNNModel(cnn_exec=cnn_exec, symbol=cnn, data=data, label=label, param_blocks=param_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer rmsprop\n",
      "maximum gradient 0.01\n",
      "learning rate (step size) 0.005\n",
      "epochs to train for 10\n",
      "Without using fastText Embeddings:Iter [0] Train: Time: 19.464s, Training Accuracy: 86.500             --- Dev Accuracy thus far: 87.333\n",
      "Without using fastText Embeddings:Iter [1] Train: Time: 12.501s, Training Accuracy: 89.667             --- Dev Accuracy thus far: 88.000\n",
      "Without using fastText Embeddings:Iter [2] Train: Time: 16.442s, Training Accuracy: 94.667             --- Dev Accuracy thus far: 87.333\n",
      "Without using fastText Embeddings:Iter [3] Train: Time: 12.525s, Training Accuracy: 97.250             --- Dev Accuracy thus far: 88.000\n",
      "Without using fastText Embeddings:Iter [4] Train: Time: 12.715s, Training Accuracy: 98.417             --- Dev Accuracy thus far: 90.667\n",
      "Without using fastText Embeddings:Iter [5] Train: Time: 15.118s, Training Accuracy: 99.333             --- Dev Accuracy thus far: 89.333\n",
      "Without using fastText Embeddings:Iter [6] Train: Time: 15.928s, Training Accuracy: 99.583             --- Dev Accuracy thus far: 90.667\n",
      "Without using fastText Embeddings:Iter [7] Train: Time: 17.295s, Training Accuracy: 99.583             --- Dev Accuracy thus far: 90.333\n",
      "Without using fastText Embeddings:Iter [8] Train: Time: 16.257s, Training Accuracy: 99.583             --- Dev Accuracy thus far: 90.333\n",
      "Saved checkpoint to ./cnn-0009.params\n",
      "Without using fastText Embeddings:Iter [9] Train: Time: 10.941s, Training Accuracy: 99.583             --- Dev Accuracy thus far: 90.667\n"
     ]
    }
   ],
   "source": [
    "optimizer = 'rmsprop'\n",
    "max_grad_norm = 0.01\n",
    "learning_rate = 0.005\n",
    "epoch = 10\n",
    "predicted=[]\n",
    "actual=[]\n",
    "max_act=[]\n",
    "print('optimizer', optimizer)\n",
    "print('maximum gradient', max_grad_norm)\n",
    "print('learning rate (step size)', learning_rate)\n",
    "print('epochs to train for', epoch)\n",
    "\n",
    "# create optimizer\n",
    "opt = mx.optimizer.create(optimizer)\n",
    "opt.lr = learning_rate\n",
    "\n",
    "updater = mx.optimizer.get_updater(opt)\n",
    "\n",
    "# For each training epoch\n",
    "for iteration in range(epoch):\n",
    "    tic = time.time()\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "\n",
    "    # Over each batch of training data\n",
    "    for begin in range(0, x_train.shape[0], batch_size):\n",
    "        batchX = x_train[begin:begin+batch_size]\n",
    "        batchY = y_train[begin:begin+batch_size]\n",
    "        if batchX.shape[0] != batch_size:\n",
    "            continue\n",
    "\n",
    "        cnn_model.data[:] = batchX\n",
    "        cnn_model.label[:] = batchY\n",
    "\n",
    "        # forward\n",
    "        cnn_model.cnn_exec.forward(is_train=True)\n",
    "\n",
    "        # backward\n",
    "        cnn_model.cnn_exec.backward()\n",
    "\n",
    "        # eval on training data\n",
    "        num_correct += sum(batchY == np.argmax(cnn_model.cnn_exec.outputs[0].asnumpy(), axis=1))\n",
    "        num_total += len(batchY)\n",
    "        \n",
    "\n",
    "        # update weights\n",
    "        norm = 0\n",
    "        for idx, weight, grad, name in cnn_model.param_blocks:\n",
    "            grad /= batch_size\n",
    "            l2_norm = mx.nd.norm(grad).asscalar()\n",
    "            norm += l2_norm * l2_norm\n",
    "\n",
    "        norm = math.sqrt(norm)\n",
    "        for idx, weight, grad, name in cnn_model.param_blocks:\n",
    "            if norm > max_grad_norm:\n",
    "                grad *= (max_grad_norm / norm)\n",
    "\n",
    "            updater(idx, grad, weight)\n",
    "\n",
    "            # reset gradient to zero\n",
    "            grad[:] = 0.0\n",
    "\n",
    "    # Decay learning rate for this epoch to ensure we are not \"overshooting\" optima\n",
    "    if iteration % 50 == 0 and iteration > 0:\n",
    "        opt.lr *= 0.5\n",
    "        print('reset learning rate to %g' % opt.lr)\n",
    "\n",
    "    # End of training loop for this epoch\n",
    "    toc = time.time()\n",
    "    train_time = toc - tic\n",
    "    train_acc = num_correct * 100 / float(num_total)\n",
    "\n",
    "    # Saving checkpoint to disk\n",
    "    if (iteration + 1) % 10 == 0:\n",
    "        prefix = 'cnn'\n",
    "        cnn_model.symbol.save('./%s-symbol.json' % prefix)\n",
    "        save_dict = {('arg:%s' % k) : v  for k, v in cnn_model.cnn_exec.arg_dict.items()}\n",
    "        save_dict.update({('aux:%s' % k) : v for k, v in cnn_model.cnn_exec.aux_dict.items()})\n",
    "        param_name = './%s-%04d.params' % (prefix, iteration)\n",
    "        mx.nd.save(param_name, save_dict)\n",
    "        print('Saved checkpoint to %s' % param_name)\n",
    "\n",
    "\n",
    "    # Evaluate model after this epoch on dev (test) set\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "\n",
    "    # For each test batch\n",
    "    for begin in range(0, x_test.shape[0], batch_size):\n",
    "        batchX = x_test[begin:begin+batch_size]\n",
    "        batchY = y_test[begin:begin+batch_size]\n",
    "\n",
    "        if batchX.shape[0] != batch_size:\n",
    "            continue\n",
    "\n",
    "        cnn_model.data[:] = batchX\n",
    "        cnn_model.cnn_exec.forward(is_train=False)\n",
    "\n",
    "        num_correct += sum(batchY == np.argmax(cnn_model.cnn_exec.outputs[0].asnumpy(), axis=1))\n",
    "        predicted.append(cnn_model.cnn_exec.outputs[0].asnumpy())\n",
    "        actual.append(batchY)\n",
    "        max_act.append(np.argmax(cnn_model.cnn_exec.outputs[0].asnumpy(), axis=1))\n",
    "        num_total += len(batchY)\n",
    "\n",
    "    dev_acc = num_correct * 100 / float(num_total)\n",
    "    print('Without using fastText Embeddings:Iter [%d] Train: Time: %.3fs, Training Accuracy: %.3f \\\n",
    "            --- Dev Accuracy thus far: %.3f' % (iteration, train_time, train_acc, dev_acc))\n",
    "    #epoch.end.callback = mx.callback.early.stop(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('f1', 0.45608108108108103)\n"
     ]
    }
   ],
   "source": [
    "t=[]\n",
    "j=[]\n",
    "l=[]\n",
    "for pred in predicted:\n",
    "    for i in pred:\n",
    "        t.append(i)\n",
    "for act in actual:\n",
    "    for i in act:\n",
    "        j.append(i)\n",
    "for maxx in max_act:\n",
    "    for i in maxx:\n",
    "        l.append(i)\n",
    "eval_metrics_2 = mx.metric.F1()\n",
    "eval_metrics_2.update(labels = mx.nd.array(j), preds = mx.nd.array(t))\n",
    "print(eval_metrics_2.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1-score gives us correct view of the classification. Since, the dataset is skewed towards negative than positive, hence though accuracy is high, its actually because of how accuracy is actually calculated where we consider Accuracy=(TP + TN) / (TP+TN+FP+FN), hence if even true positives are 0, TN can dominate. This is a well known case of \"Accuracy Paradox\" Thus we use F1 score= (1/precision)+(1/recall)\"F1-score gives us correct view of the classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Gradient normalization value 0f 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size 20\n",
      "embedding dimensions 200\n",
      "convolution filters [2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "print('batch size', batch_size)\n",
    "\n",
    "input_x = mx.sym.Variable('data') # placeholder for input data\n",
    "input_y = mx.sym.Variable('softmax_label') # placeholder for output label\n",
    "num_embed = 200 # dimensions to embed words into\n",
    "print('embedding dimensions', num_embed)\n",
    "\n",
    "embed_layer = mx.sym.Embedding(data=input_x, input_dim=vocab_size, output_dim=num_embed, name='vocab_embed')\n",
    "\n",
    "# reshape embedded data for next layer\n",
    "conv_input = mx.sym.Reshape(data=embed_layer, shape=(batch_size, 1, sentence_size, num_embed))\n",
    "filter_list=[2,3, 4, 5] # the size of filters to use\n",
    "print('convolution filters', filter_list)\n",
    "\n",
    "num_filter=100\n",
    "pooled_outputs = []\n",
    "for filter_size in filter_list:\n",
    "    convi = mx.sym.Convolution(data=conv_input, kernel=(filter_size, num_embed), num_filter=num_filter)\n",
    "    relui = mx.sym.Activation(data=convi, act_type='relu')\n",
    "    pooli = mx.sym.Pooling(data=relui, pool_type='max', kernel=(sentence_size - filter_size + 1, 1), stride=(1, 1))\n",
    "    pooled_outputs.append(pooli)\n",
    "\n",
    "# combine all pooled outputs\n",
    "total_filters = num_filter * len(filter_list)\n",
    "concat = mx.sym.Concat(*pooled_outputs, dim=1)\n",
    "\n",
    "# reshape for next layer\n",
    "h_pool = mx.sym.Reshape(data=concat, shape=(batch_size, total_filters))\n",
    "\n",
    "num_label = 2\n",
    "\n",
    "cls_weight = mx.sym.Variable('cls_weight')\n",
    "cls_bias = mx.sym.Variable('cls_bias')\n",
    "\n",
    "fc = mx.sym.FullyConnected(data=h_pool, weight=cls_weight, bias=cls_bias, num_hidden=num_label)\n",
    "\n",
    "# softmax output\n",
    "sm = mx.sym.SoftmaxOutput(data=fc, label=input_y, name='softmax')\n",
    "\n",
    "# set CNN pointer to the \"back\" of the network\n",
    "cnn = sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Define the structure of our CNN Model (as a named tuple)\n",
    "CNNModel = namedtuple(\"CNNModel\", ['cnn_exec', 'symbol', 'data', 'label', 'param_blocks'])\n",
    "\n",
    "# Define what device to train/test on, use GPU if available\n",
    "ctx = mx.cpu()\n",
    "arg_names = cnn.list_arguments()\n",
    "\n",
    "input_shapes = {}\n",
    "input_shapes['data'] = (batch_size, sentence_size)\n",
    "\n",
    "arg_shape, out_shape, aux_shape = cnn.infer_shape(**input_shapes)\n",
    "arg_arrays = [mx.nd.zeros(s, ctx) for s in arg_shape]\n",
    "args_grad = {}\n",
    "for shape, name in zip(arg_shape, arg_names):\n",
    "    if name in ['softmax_label', 'data']: # input, output\n",
    "        continue\n",
    "    args_grad[name] = mx.nd.zeros(shape, ctx)\n",
    "\n",
    "cnn_exec = cnn.bind(ctx=ctx, args=arg_arrays, args_grad=args_grad, grad_req='add')\n",
    "\n",
    "param_blocks = []\n",
    "arg_dict = dict(zip(arg_names, cnn_exec.arg_arrays))\n",
    "initializer = mx.initializer.Uniform(0.1)\n",
    "for i, name in enumerate(arg_names):\n",
    "    if name in ['softmax_label', 'data']: # input, output\n",
    "        continue\n",
    "    initializer(mx.init.InitDesc(name), arg_dict[name])\n",
    "\n",
    "    param_blocks.append( (i, arg_dict[name], args_grad[name], name) )\n",
    "\n",
    "data = cnn_exec.arg_dict['data']\n",
    "label = cnn_exec.arg_dict['softmax_label']\n",
    "\n",
    "cnn_model= CNNModel(cnn_exec=cnn_exec, symbol=cnn, data=data, label=label, param_blocks=param_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer rmsprop\n",
      "maximum gradient 0.5\n",
      "learning rate (step size) 0.005\n",
      "epochs to train for 10\n",
      "Iter [0] Train: Time: 12.666s, Training Accuracy: 85.750             --- Dev Accuracy thus far: 90.333\n",
      "Iter [1] Train: Time: 12.651s, Training Accuracy: 96.417             --- Dev Accuracy thus far: 91.667\n",
      "Iter [2] Train: Time: 12.724s, Training Accuracy: 99.000             --- Dev Accuracy thus far: 90.000\n",
      "Iter [3] Train: Time: 12.361s, Training Accuracy: 99.417             --- Dev Accuracy thus far: 90.667\n",
      "Iter [4] Train: Time: 9.742s, Training Accuracy: 99.833             --- Dev Accuracy thus far: 90.000\n",
      "Iter [5] Train: Time: 9.770s, Training Accuracy: 99.750             --- Dev Accuracy thus far: 90.000\n",
      "Iter [6] Train: Time: 9.739s, Training Accuracy: 99.833             --- Dev Accuracy thus far: 89.333\n",
      "Iter [7] Train: Time: 9.753s, Training Accuracy: 99.917             --- Dev Accuracy thus far: 90.667\n",
      "Iter [8] Train: Time: 9.790s, Training Accuracy: 99.833             --- Dev Accuracy thus far: 91.000\n",
      "Saved checkpoint to ./cnn-0009.params\n",
      "Iter [9] Train: Time: 9.739s, Training Accuracy: 99.833             --- Dev Accuracy thus far: 90.667\n"
     ]
    }
   ],
   "source": [
    "optimizer = 'rmsprop'\n",
    "max_grad_norm = 0.5\n",
    "learning_rate = 0.005\n",
    "epoch = 10\n",
    "predicted=[]\n",
    "actual=[]\n",
    "max_act=[]\n",
    "print('optimizer', optimizer)\n",
    "print('maximum gradient', max_grad_norm)\n",
    "print('learning rate (step size)', learning_rate)\n",
    "print('epochs to train for', epoch)\n",
    "\n",
    "# create optimizer\n",
    "opt = mx.optimizer.create(optimizer)\n",
    "opt.lr = learning_rate\n",
    "\n",
    "updater = mx.optimizer.get_updater(opt)\n",
    "\n",
    "# For each training epoch\n",
    "for iteration in range(epoch):\n",
    "    tic = time.time()\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "\n",
    "    # Over each batch of training data\n",
    "    for begin in range(0, x_train.shape[0], batch_size):\n",
    "        batchX = x_train[begin:begin+batch_size]\n",
    "        batchY = y_train[begin:begin+batch_size]\n",
    "        if batchX.shape[0] != batch_size:\n",
    "            continue\n",
    "\n",
    "        cnn_model.data[:] = batchX\n",
    "        cnn_model.label[:] = batchY\n",
    "\n",
    "        # forward\n",
    "        cnn_model.cnn_exec.forward(is_train=True)\n",
    "\n",
    "        # backward\n",
    "        cnn_model.cnn_exec.backward()\n",
    "\n",
    "        # eval on training data\n",
    "        num_correct += sum(batchY == np.argmax(cnn_model.cnn_exec.outputs[0].asnumpy(), axis=1))\n",
    "        num_total += len(batchY)\n",
    "        \n",
    "\n",
    "        # update weights\n",
    "        norm = 0\n",
    "        for idx, weight, grad, name in cnn_model.param_blocks:\n",
    "            grad /= batch_size\n",
    "            l2_norm = mx.nd.norm(grad).asscalar()\n",
    "            norm += l2_norm * l2_norm\n",
    "\n",
    "        norm = math.sqrt(norm)\n",
    "        for idx, weight, grad, name in cnn_model.param_blocks:\n",
    "            if norm > max_grad_norm:\n",
    "                grad *= (max_grad_norm / norm)\n",
    "\n",
    "            updater(idx, grad, weight)\n",
    "\n",
    "            # reset gradient to zero\n",
    "            grad[:] = 0.0\n",
    "\n",
    "    # Decay learning rate for this epoch to ensure we are not \"overshooting\" optima\n",
    "    if iteration % 50 == 0 and iteration > 0:\n",
    "        opt.lr *= 0.5\n",
    "        print('reset learning rate to %g' % opt.lr)\n",
    "\n",
    "    # End of training loop for this epoch\n",
    "    toc = time.time()\n",
    "    train_time = toc - tic\n",
    "    train_acc = num_correct * 100 / float(num_total)\n",
    "\n",
    "    # Saving checkpoint to disk\n",
    "    if (iteration + 1) % 10 == 0:\n",
    "        prefix = 'cnn'\n",
    "        cnn_model.symbol.save('./%s-symbol.json' % prefix)\n",
    "        save_dict = {('arg:%s' % k) : v  for k, v in cnn_model.cnn_exec.arg_dict.items()}\n",
    "        save_dict.update({('aux:%s' % k) : v for k, v in cnn_model.cnn_exec.aux_dict.items()})\n",
    "        param_name = './%s-%04d.params' % (prefix, iteration)\n",
    "        mx.nd.save(param_name, save_dict)\n",
    "        print('Saved checkpoint to %s' % param_name)\n",
    "\n",
    "\n",
    "    # Evaluate model after this epoch on dev (test) set\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "\n",
    "    # For each test batch\n",
    "    for begin in range(0, x_test.shape[0], batch_size):\n",
    "        batchX = x_test[begin:begin+batch_size]\n",
    "        batchY = y_test[begin:begin+batch_size]\n",
    "\n",
    "        if batchX.shape[0] != batch_size:\n",
    "            continue\n",
    "\n",
    "        cnn_model.data[:] = batchX\n",
    "        cnn_model.cnn_exec.forward(is_train=False)\n",
    "\n",
    "        num_correct += sum(batchY == np.argmax(cnn_model.cnn_exec.outputs[0].asnumpy(), axis=1))\n",
    "        num_total += len(batchY)\n",
    "        predicted.append(cnn_model.cnn_exec.outputs[0].asnumpy())\n",
    "        actual.append(batchY)\n",
    "        max_act.append(np.argmax(cnn_model.cnn_exec.outputs[0].asnumpy(), axis=1))\n",
    "\n",
    "    dev_acc = num_correct * 100 / float(num_total)\n",
    "    print('Iter [%d] Train: Time: %.3fs, Training Accuracy: %.3f \\\n",
    "            --- Dev Accuracy thus far: %.3f' % (iteration, train_time, train_acc, dev_acc))\n",
    "    #epoch.end.callback = mx.callback.early.stop(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score on Gradient Normalization of 0.5: ('f1', 0.5618320610687023)\n"
     ]
    }
   ],
   "source": [
    "t=[]\n",
    "j=[]\n",
    "l=[]\n",
    "for pred in predicted:\n",
    "    for i in pred:\n",
    "        t.append(i)\n",
    "for act in actual:\n",
    "    for i in act:\n",
    "        j.append(i)\n",
    "for maxx in max_act:\n",
    "    for i in maxx:\n",
    "        l.append(i)\n",
    "eval_metrics_2 = mx.metric.F1()\n",
    "eval_metrics_2.update(labels = mx.nd.array(j), preds = mx.nd.array(t))\n",
    "print(\"f1 score on Gradient Normalization of 0.5:\",eval_metrics_2.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size 20\n",
      "embedding dimensions 200\n",
      "convolution filters [2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "print('batch size', batch_size)\n",
    "\n",
    "input_x = mx.sym.Variable('data') # placeholder for input data\n",
    "input_y = mx.sym.Variable('softmax_label') # placeholder for output label\n",
    "num_embed = 200 # dimensions to embed words into\n",
    "print('embedding dimensions', num_embed)\n",
    "\n",
    "embed_layer = mx.sym.Embedding(data=input_x, input_dim=vocab_size, output_dim=num_embed, name='vocab_embed')\n",
    "\n",
    "# reshape embedded data for next layer\n",
    "conv_input = mx.sym.Reshape(data=embed_layer, shape=(batch_size, 1, sentence_size, num_embed))\n",
    "filter_list=[2,3, 4, 5] # the size of filters to use\n",
    "print('convolution filters', filter_list)\n",
    "\n",
    "num_filter=100\n",
    "pooled_outputs = []\n",
    "for filter_size in filter_list:\n",
    "    convi = mx.sym.Convolution(data=conv_input, kernel=(filter_size, num_embed), num_filter=num_filter)\n",
    "    relui = mx.sym.Activation(data=convi, act_type='relu')\n",
    "    pooli = mx.sym.Pooling(data=relui, pool_type='max', kernel=(sentence_size - filter_size + 1, 1), stride=(1, 1))\n",
    "    pooled_outputs.append(pooli)\n",
    "\n",
    "# combine all pooled outputs\n",
    "total_filters = num_filter * len(filter_list)\n",
    "concat = mx.sym.Concat(*pooled_outputs, dim=1)\n",
    "\n",
    "# reshape for next layer\n",
    "h_pool = mx.sym.Reshape(data=concat, shape=(batch_size, total_filters))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.5\n",
    "#print('dropout probability', dropout)\n",
    "\n",
    "if dropout > 0.0:\n",
    "    h_drop = mx.sym.Dropout(data=h_pool, p=dropout)\n",
    "else:\n",
    "    h_drop = h_pool\n",
    "\n",
    "    num_label = 2\n",
    "\n",
    "cls_weight = mx.sym.Variable('cls_weight')\n",
    "cls_bias = mx.sym.Variable('cls_bias')\n",
    "\n",
    "fc = mx.sym.FullyConnected(data=h_pool, weight=cls_weight, bias=cls_bias, num_hidden=num_label)\n",
    "\n",
    "# softmax output\n",
    "sm = mx.sym.SoftmaxOutput(data=fc, label=input_y, name='softmax')\n",
    "\n",
    "# set CNN pointer to the \"back\" of the network\n",
    "cnn = sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Define the structure of our CNN Model (as a named tuple)\n",
    "CNNModel = namedtuple(\"CNNModel\", ['cnn_exec', 'symbol', 'data', 'label', 'param_blocks'])\n",
    "\n",
    "# Define what device to train/test on, use GPU if available\n",
    "ctx = mx.cpu()\n",
    "arg_names = cnn.list_arguments()\n",
    "\n",
    "input_shapes = {}\n",
    "input_shapes['data'] = (batch_size, sentence_size)\n",
    "\n",
    "arg_shape, out_shape, aux_shape = cnn.infer_shape(**input_shapes)\n",
    "arg_arrays = [mx.nd.zeros(s, ctx) for s in arg_shape]\n",
    "args_grad = {}\n",
    "for shape, name in zip(arg_shape, arg_names):\n",
    "    if name in ['softmax_label', 'data']: # input, output\n",
    "        continue\n",
    "    args_grad[name] = mx.nd.zeros(shape, ctx)\n",
    "\n",
    "cnn_exec = cnn.bind(ctx=ctx, args=arg_arrays, args_grad=args_grad, grad_req='add')\n",
    "\n",
    "param_blocks = []\n",
    "arg_dict = dict(zip(arg_names, cnn_exec.arg_arrays))\n",
    "initializer = mx.initializer.Uniform(0.1)\n",
    "for i, name in enumerate(arg_names):\n",
    "    if name in ['softmax_label', 'data']: # input, output\n",
    "        continue\n",
    "    initializer(mx.init.InitDesc(name), arg_dict[name])\n",
    "\n",
    "    param_blocks.append( (i, arg_dict[name], args_grad[name], name) )\n",
    "\n",
    "data = cnn_exec.arg_dict['data']\n",
    "label = cnn_exec.arg_dict['softmax_label']\n",
    "\n",
    "cnn_model= CNNModel(cnn_exec=cnn_exec, symbol=cnn, data=data, label=label, param_blocks=param_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer rmsprop\n",
      "maximum gradient 0.01\n",
      "learning rate (step size) 0.005\n",
      "epochs to train for 10\n",
      "Iter [0] Train: Time: 12.600s, Training Accuracy: 85.417             --- Dev Accuracy thus far: 87.333\n",
      "Iter [1] Train: Time: 11.501s, Training Accuracy: 89.583             --- Dev Accuracy thus far: 88.000\n",
      "Iter [2] Train: Time: 11.804s, Training Accuracy: 94.833             --- Dev Accuracy thus far: 88.667\n",
      "Iter [3] Train: Time: 13.753s, Training Accuracy: 97.083             --- Dev Accuracy thus far: 89.333\n",
      "Iter [4] Train: Time: 13.704s, Training Accuracy: 98.583             --- Dev Accuracy thus far: 90.667\n",
      "Iter [5] Train: Time: 10.590s, Training Accuracy: 99.250             --- Dev Accuracy thus far: 89.667\n",
      "Iter [6] Train: Time: 10.244s, Training Accuracy: 99.583             --- Dev Accuracy thus far: 89.333\n",
      "Iter [7] Train: Time: 10.088s, Training Accuracy: 99.583             --- Dev Accuracy thus far: 89.667\n",
      "Iter [8] Train: Time: 11.303s, Training Accuracy: 99.667             --- Dev Accuracy thus far: 89.667\n",
      "Saved checkpoint to ./cnn-0009.params\n",
      "Iter [9] Train: Time: 11.622s, Training Accuracy: 99.750             --- Dev Accuracy thus far: 89.667\n"
     ]
    }
   ],
   "source": [
    "optimizer = 'rmsprop'\n",
    "max_grad_norm = 0.01\n",
    "learning_rate = 0.005\n",
    "epoch = 10\n",
    "predicted=[]\n",
    "actual=[]\n",
    "max_act=[]\n",
    "print('optimizer', optimizer)\n",
    "print('maximum gradient', max_grad_norm)\n",
    "print('learning rate (step size)', learning_rate)\n",
    "print('epochs to train for', epoch)\n",
    "\n",
    "# create optimizer\n",
    "opt = mx.optimizer.create(optimizer)\n",
    "opt.lr = learning_rate\n",
    "\n",
    "updater = mx.optimizer.get_updater(opt)\n",
    "\n",
    "# For each training epoch\n",
    "for iteration in range(epoch):\n",
    "    tic = time.time()\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "\n",
    "    # Over each batch of training data\n",
    "    for begin in range(0, x_train.shape[0], batch_size):\n",
    "        batchX = x_train[begin:begin+batch_size]\n",
    "        batchY = y_train[begin:begin+batch_size]\n",
    "        if batchX.shape[0] != batch_size:\n",
    "            continue\n",
    "\n",
    "        cnn_model.data[:] = batchX\n",
    "        cnn_model.label[:] = batchY\n",
    "\n",
    "        # forward\n",
    "        cnn_model.cnn_exec.forward(is_train=True)\n",
    "\n",
    "        # backward\n",
    "        cnn_model.cnn_exec.backward()\n",
    "\n",
    "        # eval on training data\n",
    "        num_correct += sum(batchY == np.argmax(cnn_model.cnn_exec.outputs[0].asnumpy(), axis=1))\n",
    "        num_total += len(batchY)\n",
    "        \n",
    "\n",
    "        # update weights\n",
    "        norm = 0\n",
    "        for idx, weight, grad, name in cnn_model.param_blocks:\n",
    "            grad /= batch_size\n",
    "            l2_norm = mx.nd.norm(grad).asscalar()\n",
    "            norm += l2_norm * l2_norm\n",
    "\n",
    "        norm = math.sqrt(norm)\n",
    "        for idx, weight, grad, name in cnn_model.param_blocks:\n",
    "            if norm > max_grad_norm:\n",
    "                grad *= (max_grad_norm / norm)\n",
    "\n",
    "            updater(idx, grad, weight)\n",
    "\n",
    "            # reset gradient to zero\n",
    "            grad[:] = 0.0\n",
    "\n",
    "    # Decay learning rate for this epoch to ensure we are not \"overshooting\" optima\n",
    "    if iteration % 50 == 0 and iteration > 0:\n",
    "        opt.lr *= 0.5\n",
    "        print('reset learning rate to %g' % opt.lr)\n",
    "\n",
    "    # End of training loop for this epoch\n",
    "    toc = time.time()\n",
    "    train_time = toc - tic\n",
    "    train_acc = num_correct * 100 / float(num_total)\n",
    "\n",
    "    # Saving checkpoint to disk\n",
    "    if (iteration + 1) % 10 == 0:\n",
    "        prefix = 'cnn'\n",
    "        cnn_model.symbol.save('./%s-symbol.json' % prefix)\n",
    "        save_dict = {('arg:%s' % k) : v  for k, v in cnn_model.cnn_exec.arg_dict.items()}\n",
    "        save_dict.update({('aux:%s' % k) : v for k, v in cnn_model.cnn_exec.aux_dict.items()})\n",
    "        param_name = './%s-%04d.params' % (prefix, iteration)\n",
    "        mx.nd.save(param_name, save_dict)\n",
    "        print('Saved checkpoint to %s' % param_name)\n",
    "\n",
    "\n",
    "    # Evaluate model after this epoch on dev (test) set\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "\n",
    "    # For each test batch\n",
    "    for begin in range(0, x_test.shape[0], batch_size):\n",
    "        batchX = x_test[begin:begin+batch_size]\n",
    "        batchY = y_test[begin:begin+batch_size]\n",
    "\n",
    "        if batchX.shape[0] != batch_size:\n",
    "            continue\n",
    "\n",
    "        cnn_model.data[:] = batchX\n",
    "        cnn_model.cnn_exec.forward(is_train=False)\n",
    "\n",
    "        num_correct += sum(batchY == np.argmax(cnn_model.cnn_exec.outputs[0].asnumpy(), axis=1))\n",
    "        num_total += len(batchY)\n",
    "        predicted.append(cnn_model.cnn_exec.outputs[0].asnumpy())\n",
    "        actual.append(batchY)\n",
    "        max_act.append(np.argmax(cnn_model.cnn_exec.outputs[0].asnumpy(), axis=1))\n",
    "\n",
    "    dev_acc = num_correct * 100 / float(num_total)\n",
    "    print('Iter [%d] Train: Time: %.3fs, Training Accuracy: %.3f \\\n",
    "            --- Dev Accuracy thus far: %.3f' % (iteration, train_time, train_acc, dev_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score after using Dropout: ('f1', 0.4375)\n"
     ]
    }
   ],
   "source": [
    "t=[]\n",
    "j=[]\n",
    "l=[]\n",
    "for pred in predicted:\n",
    "    for i in pred:\n",
    "        t.append(i)\n",
    "for act in actual:\n",
    "    for i in act:\n",
    "        j.append(i)\n",
    "for maxx in max_act:\n",
    "    for i in maxx:\n",
    "        l.append(i)\n",
    "eval_metrics_2 = mx.metric.F1()\n",
    "eval_metrics_2.update(labels = mx.nd.array(j), preds = mx.nd.array(t))\n",
    "print(\"f1 score after using Dropout:\",eval_metrics_2.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doing Xavier Initialization of the parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size 20\n",
      "embedding dimensions 200\n",
      "convolution filters [2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "print('batch size', batch_size)\n",
    "\n",
    "input_x = mx.sym.Variable('data') # placeholder for input data\n",
    "input_y = mx.sym.Variable('softmax_label') # placeholder for output label\n",
    "num_embed = 200 # dimensions to embed words into\n",
    "print('embedding dimensions', num_embed)\n",
    "\n",
    "embed_layer = mx.sym.Embedding(data=input_x, input_dim=vocab_size, output_dim=num_embed, name='vocab_embed')\n",
    "\n",
    "# reshape embedded data for next layer\n",
    "conv_input = mx.sym.Reshape(data=embed_layer, shape=(batch_size, 1, sentence_size, num_embed))\n",
    "filter_list=[2,3, 4, 5] # the size of filters to use\n",
    "print('convolution filters', filter_list)\n",
    "\n",
    "num_filter=100\n",
    "pooled_outputs = []\n",
    "for filter_size in filter_list:\n",
    "    convi = mx.sym.Convolution(data=conv_input, kernel=(filter_size, num_embed), num_filter=num_filter)\n",
    "    relui = mx.sym.Activation(data=convi, act_type='relu')\n",
    "    pooli = mx.sym.Pooling(data=relui, pool_type='max', kernel=(sentence_size - filter_size + 1, 1), stride=(1, 1))\n",
    "    pooled_outputs.append(pooli)\n",
    "\n",
    "# combine all pooled outputs\n",
    "total_filters = num_filter * len(filter_list)\n",
    "concat = mx.sym.Concat(*pooled_outputs, dim=1)\n",
    "\n",
    "# reshape for next layer\n",
    "h_pool = mx.sym.Reshape(data=concat, shape=(batch_size, total_filters))\n",
    "\n",
    "num_label = 2\n",
    "\n",
    "cls_weight = mx.sym.Variable('cls_weight')\n",
    "cls_bias = mx.sym.Variable('cls_bias')\n",
    "\n",
    "fc = mx.sym.FullyConnected(data=h_pool, weight=cls_weight, bias=cls_bias, num_hidden=num_label)\n",
    "\n",
    "# softmax output\n",
    "sm = mx.sym.SoftmaxOutput(data=fc, label=input_y, name='softmax')\n",
    "\n",
    "# set CNN pointer to the \"back\" of the network\n",
    "cnn = sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Define the structure of our CNN Model (as a named tuple)\n",
    "CNNModel = namedtuple(\"CNNModel\", ['cnn_exec', 'symbol', 'data', 'label', 'param_blocks'])\n",
    "\n",
    "# Define what device to train/test on, use GPU if available\n",
    "ctx = mx.cpu()\n",
    "arg_names = cnn.list_arguments()\n",
    "\n",
    "input_shapes = {}\n",
    "input_shapes['data'] = (batch_size, sentence_size)\n",
    "\n",
    "arg_shape, out_shape, aux_shape = cnn.infer_shape(**input_shapes)\n",
    "arg_arrays = [mx.nd.zeros(s, ctx) for s in arg_shape]\n",
    "args_grad = {}\n",
    "for shape, name in zip(arg_shape, arg_names):\n",
    "    if name in ['softmax_label', 'data']: # input, output\n",
    "        continue\n",
    "    args_grad[name] = mx.nd.zeros(shape, ctx)\n",
    "\n",
    "cnn_exec = cnn.bind(ctx=ctx, args=arg_arrays, args_grad=args_grad, grad_req='add')\n",
    "\n",
    "param_blocks = []\n",
    "arg_dict = dict(zip(arg_names, cnn_exec.arg_arrays))\n",
    "initializer = mx.initializer.Xavier(rnd_type='uniform', factor_type='avg', magnitude=3)\n",
    "for i, name in enumerate(arg_names):\n",
    "    if name in ['softmax_label', 'data']: # input, output\n",
    "        continue\n",
    "    initializer(mx.init.InitDesc(name), arg_dict[name])\n",
    "\n",
    "    param_blocks.append( (i, arg_dict[name], args_grad[name], name) )\n",
    "\n",
    "data = cnn_exec.arg_dict['data']\n",
    "label = cnn_exec.arg_dict['softmax_label']\n",
    "\n",
    "cnn_model= CNNModel(cnn_exec=cnn_exec, symbol=cnn, data=data, label=label, param_blocks=param_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer rmsprop\n",
      "maximum gradient 0.01\n",
      "learning rate (step size) 0.005\n",
      "epochs to train for 10\n",
      "Iter [0] Train: Time: 10.373s, Training Accuracy: 85.333             --- Dev Accuracy thus far: 86.000\n",
      "Iter [1] Train: Time: 9.484s, Training Accuracy: 86.333             --- Dev Accuracy thus far: 87.667\n",
      "Iter [2] Train: Time: 9.558s, Training Accuracy: 89.583             --- Dev Accuracy thus far: 85.667\n",
      "Iter [3] Train: Time: 10.065s, Training Accuracy: 92.250             --- Dev Accuracy thus far: 85.000\n",
      "Iter [4] Train: Time: 19.531s, Training Accuracy: 95.333             --- Dev Accuracy thus far: 85.333\n",
      "Iter [5] Train: Time: 19.444s, Training Accuracy: 97.083             --- Dev Accuracy thus far: 86.000\n",
      "Iter [6] Train: Time: 19.523s, Training Accuracy: 97.750             --- Dev Accuracy thus far: 85.333\n",
      "Iter [7] Train: Time: 19.525s, Training Accuracy: 98.417             --- Dev Accuracy thus far: 85.333\n",
      "Iter [8] Train: Time: 18.873s, Training Accuracy: 98.750             --- Dev Accuracy thus far: 87.000\n",
      "Saved checkpoint to ./cnn-0009.params\n",
      "Iter [9] Train: Time: 12.704s, Training Accuracy: 98.667             --- Dev Accuracy thus far: 84.000\n"
     ]
    }
   ],
   "source": [
    "optimizer = 'rmsprop'\n",
    "max_grad_norm = 0.01\n",
    "learning_rate = 0.005\n",
    "epoch = 10\n",
    "predicted=[]\n",
    "actual=[]\n",
    "max_act=[]\n",
    "print('optimizer', optimizer)\n",
    "print('maximum gradient', max_grad_norm)\n",
    "print('learning rate (step size)', learning_rate)\n",
    "print('epochs to train for', epoch)\n",
    "\n",
    "# create optimizer\n",
    "opt = mx.optimizer.create(optimizer)\n",
    "opt.lr = learning_rate\n",
    "\n",
    "updater = mx.optimizer.get_updater(opt)\n",
    "\n",
    "# For each training epoch\n",
    "for iteration in range(epoch):\n",
    "    tic = time.time()\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "\n",
    "    # Over each batch of training data\n",
    "    for begin in range(0, x_train.shape[0], batch_size):\n",
    "        batchX = x_train[begin:begin+batch_size]\n",
    "        batchY = y_train[begin:begin+batch_size]\n",
    "        if batchX.shape[0] != batch_size:\n",
    "            continue\n",
    "\n",
    "        cnn_model.data[:] = batchX\n",
    "        cnn_model.label[:] = batchY\n",
    "\n",
    "        # forward\n",
    "        cnn_model.cnn_exec.forward(is_train=True)\n",
    "\n",
    "        # backward\n",
    "        cnn_model.cnn_exec.backward()\n",
    "\n",
    "        # eval on training data\n",
    "        num_correct += sum(batchY == np.argmax(cnn_model.cnn_exec.outputs[0].asnumpy(), axis=1))\n",
    "        num_total += len(batchY)\n",
    "        \n",
    "\n",
    "        # update weights\n",
    "        norm = 0\n",
    "        for idx, weight, grad, name in cnn_model.param_blocks:\n",
    "            grad /= batch_size\n",
    "            l2_norm = mx.nd.norm(grad).asscalar()\n",
    "            norm += l2_norm * l2_norm\n",
    "\n",
    "        norm = math.sqrt(norm)\n",
    "        for idx, weight, grad, name in cnn_model.param_blocks:\n",
    "            if norm > max_grad_norm:\n",
    "                grad *= (max_grad_norm / norm)\n",
    "\n",
    "            updater(idx, grad, weight)\n",
    "\n",
    "            # reset gradient to zero\n",
    "            grad[:] = 0.0\n",
    "\n",
    "    # Decay learning rate for this epoch to ensure we are not \"overshooting\" optima\n",
    "    if iteration % 50 == 0 and iteration > 0:\n",
    "        opt.lr *= 0.5\n",
    "        print('reset learning rate to %g' % opt.lr)\n",
    "\n",
    "    # End of training loop for this epoch\n",
    "    toc = time.time()\n",
    "    train_time = toc - tic\n",
    "    train_acc = num_correct * 100 / float(num_total)\n",
    "\n",
    "    # Saving checkpoint to disk\n",
    "    if (iteration + 1) % 10 == 0:\n",
    "        prefix = 'cnn'\n",
    "        cnn_model.symbol.save('./%s-symbol.json' % prefix)\n",
    "        save_dict = {('arg:%s' % k) : v  for k, v in cnn_model.cnn_exec.arg_dict.items()}\n",
    "        save_dict.update({('aux:%s' % k) : v for k, v in cnn_model.cnn_exec.aux_dict.items()})\n",
    "        param_name = './%s-%04d.params' % (prefix, iteration)\n",
    "        mx.nd.save(param_name, save_dict)\n",
    "        print('Saved checkpoint to %s' % param_name)\n",
    "\n",
    "\n",
    "    # Evaluate model after this epoch on dev (test) set\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "\n",
    "    # For each test batch\n",
    "    for begin in range(0, x_test.shape[0], batch_size):\n",
    "        batchX = x_test[begin:begin+batch_size]\n",
    "        batchY = y_test[begin:begin+batch_size]\n",
    "\n",
    "        if batchX.shape[0] != batch_size:\n",
    "            continue\n",
    "\n",
    "        cnn_model.data[:] = batchX\n",
    "        cnn_model.cnn_exec.forward(is_train=False)\n",
    "\n",
    "        num_correct += sum(batchY == np.argmax(cnn_model.cnn_exec.outputs[0].asnumpy(), axis=1))\n",
    "        num_total += len(batchY)\n",
    "        predicted.append(cnn_model.cnn_exec.outputs[0].asnumpy())\n",
    "        actual.append(batchY)\n",
    "        max_act.append(np.argmax(cnn_model.cnn_exec.outputs[0].asnumpy(), axis=1))\n",
    "\n",
    "    dev_acc = num_correct * 100 / float(num_total)\n",
    "    print('Iter [%d] Train: Time: %.3fs, Training Accuracy: %.3f \\\n",
    "            --- Dev Accuracy thus far: %.3f' % (iteration, train_time, train_acc, dev_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score after using Xavier Initialization: ('f1', 0.3954802259887006)\n"
     ]
    }
   ],
   "source": [
    "t=[]\n",
    "j=[]\n",
    "l=[]\n",
    "for pred in predicted:\n",
    "    for i in pred:\n",
    "        t.append(i)\n",
    "for act in actual:\n",
    "    for i in act:\n",
    "        j.append(i)\n",
    "for maxx in max_act:\n",
    "    for i in maxx:\n",
    "        l.append(i)\n",
    "eval_metrics_2 = mx.metric.F1()\n",
    "eval_metrics_2.update(labels = mx.nd.array(j), preds = mx.nd.array(t))\n",
    "print(\"f1 score after using Xavier Initialization:\",eval_metrics_2.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastText "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "with open('listfile.txt', 'w') as filehandle:  \n",
    "    for item in padded_sentences:\n",
    "        filehandle.write(str(item) + \"\\n\")\n",
    "filehandle.close()\n",
    "f1 = open('listfile.txt', 'r')\n",
    "f2 = open('file2.txt', 'w')\n",
    "for line in f1:\n",
    "    f2.write(line.replace('[', '').replace(']','').replace(',','').replace(\"'\", \"\"))\n",
    "\n",
    "f1.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.skipgram('file2.txt', 'model', lr=0.1, dim=200, ws=5,word_ngrams=6)\n",
    "model = fasttext.load_model('model.bin', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open('model.vec') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "f.close()\n",
    "embedding_matrix = np.zeros((len(model.words ) + 1, model.dim))\n",
    "i=0\n",
    "for word in model.words :\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fast = np.array([\n",
    "            [model[word] for word in sentence]\n",
    "            for sentence in padded_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fast_train, y_fast_train, x_fast_test, y_fast_test= create_shuffle(x_fast,y)\n",
    "sentence_size_fast = x_fast_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size 20\n",
      "convolution filters [2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "print('batch size', batch_size)\n",
    "\n",
    "input_x = mx.sym.Variable('data') # placeholder for input data\n",
    "input_y = mx.sym.Variable('softmax_label') # placeholder for output label\n",
    "num_embed = 200 # dimensions to embed words into\n",
    "#print('embedding dimensions', num_embed)\n",
    "#weight_matrix = mx.nd.array(embedding_matrix)\n",
    "#the_emb_3 = mx.sym.Variable('weights')\n",
    "#embed_layer = mx.sym.Embedding(data=input_x, input_dim=model.dim, output_dim=num_embed,weight=the_emb_3,name='vocab_embed')\n",
    "# reshape embedded data for next layer\n",
    "#conv_input = input_x\n",
    "conv_input = mx.sym.Reshape(data=input_x, shape=(batch_size, 1, sentence_size_fast, num_embed))\n",
    "filter_list=[2,3, 4, 5] # the size of filters to use\n",
    "print('convolution filters', filter_list)\n",
    "\n",
    "num_filter=100\n",
    "pooled_outputs = []\n",
    "for filter_size in filter_list:\n",
    "    convi = mx.sym.Convolution(data=conv_input, kernel=(filter_size, num_embed), num_filter=num_filter)\n",
    "    relui = mx.sym.Activation(data=convi, act_type='relu')\n",
    "    pooli = mx.sym.Pooling(data=relui, pool_type='max', kernel=(sentence_size_fast - filter_size + 1, 1), stride=(1, 1))\n",
    "    pooled_outputs.append(pooli)\n",
    "\n",
    "# combine all pooled outputs\n",
    "total_filters = num_filter * len(filter_list)\n",
    "concat = mx.sym.Concat(*pooled_outputs, dim=1)\n",
    "\n",
    "# reshape for next layer\n",
    "h_pool = mx.sym.Reshape(data=concat, shape=(batch_size, total_filters))\n",
    "\n",
    "num_label = 2\n",
    "\n",
    "cls_weight = mx.sym.Variable('cls_weight')\n",
    "cls_bias = mx.sym.Variable('cls_bias')\n",
    "\n",
    "fc = mx.sym.FullyConnected(data=h_pool, weight=cls_weight, bias=cls_bias, num_hidden=num_label)\n",
    "\n",
    "# softmax output\n",
    "sm = mx.sym.SoftmaxOutput(data=fc, label=input_y, name='softmax')\n",
    "\n",
    "# set CNN pointer to the \"back\" of the network\n",
    "cnn = sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Define the structure of our CNN Model (as a named tuple)\n",
    "CNNModel = namedtuple(\"CNNModel\", ['cnn_exec', 'symbol', 'data', 'label', 'param_blocks'])\n",
    "\n",
    "# Define what device to train/test on, use GPU if available\n",
    "ctx = mx.cpu()\n",
    "arg_names = cnn.list_arguments()\n",
    "\n",
    "input_shapes = {}\n",
    "input_shapes['data'] = (batch_size, sentence_size_fast,num_embed)\n",
    "\n",
    "arg_shape, out_shape, aux_shape = cnn.infer_shape(**input_shapes)\n",
    "arg_arrays = [mx.nd.zeros(s, ctx) for s in arg_shape]\n",
    "args_grad = {}\n",
    "for shape, name in zip(arg_shape, arg_names):\n",
    "    if name in ['softmax_label', 'data']: # input, output\n",
    "        continue\n",
    "    args_grad[name] = mx.nd.zeros(shape, ctx)\n",
    "\n",
    "cnn_exec = cnn.bind(ctx=ctx, args=arg_arrays, args_grad=args_grad, grad_req='add')\n",
    "\n",
    "param_blocks = []\n",
    "arg_dict = dict(zip(arg_names, cnn_exec.arg_arrays))\n",
    "initializer = mx.initializer.Uniform(0.1)\n",
    "for i, name in enumerate(arg_names):\n",
    "    if name in ['softmax_label', 'data']: # input, output\n",
    "        continue\n",
    "    initializer(mx.init.InitDesc(name), arg_dict[name])\n",
    "\n",
    "    param_blocks.append( (i, arg_dict[name], args_grad[name], name) )\n",
    "\n",
    "data = cnn_exec.arg_dict['data']\n",
    "label = cnn_exec.arg_dict['softmax_label']\n",
    "fixed_param_names =['weights']\n",
    "#weights=weight_matrix\n",
    "cnn_model= CNNModel(cnn_exec=cnn_exec, symbol=cnn, data=data, label=label, param_blocks=param_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer rmsprop\n",
      "maximum gradient 0.01\n",
      "learning rate (step size) 0.005\n",
      "epochs to train for 10\n",
      "Iter [0] Train: Time: 10.677s, Training Accuracy: 84.917             --- Dev Accuracy thus far: 85.667\n",
      "Iter [1] Train: Time: 10.271s, Training Accuracy: 85.500             --- Dev Accuracy thus far: 86.667\n",
      "Iter [2] Train: Time: 10.378s, Training Accuracy: 87.417             --- Dev Accuracy thus far: 88.000\n",
      "Iter [3] Train: Time: 7.079s, Training Accuracy: 89.417             --- Dev Accuracy thus far: 88.333\n",
      "Iter [4] Train: Time: 7.095s, Training Accuracy: 90.583             --- Dev Accuracy thus far: 88.667\n",
      "Iter [5] Train: Time: 7.128s, Training Accuracy: 91.500             --- Dev Accuracy thus far: 85.667\n",
      "Iter [6] Train: Time: 6.839s, Training Accuracy: 91.500             --- Dev Accuracy thus far: 86.333\n",
      "Iter [7] Train: Time: 6.738s, Training Accuracy: 92.917             --- Dev Accuracy thus far: 89.000\n",
      "Iter [8] Train: Time: 7.159s, Training Accuracy: 91.583             --- Dev Accuracy thus far: 86.667\n",
      "Saved checkpoint to ./cnn-0009.params\n",
      "Iter [9] Train: Time: 7.184s, Training Accuracy: 93.917             --- Dev Accuracy thus far: 81.667\n"
     ]
    }
   ],
   "source": [
    "optimizer = 'rmsprop'\n",
    "max_grad_norm = 0.01\n",
    "learning_rate = 0.005\n",
    "epoch = 10\n",
    "predicted=[]\n",
    "actual=[]\n",
    "max_act=[]\n",
    "print('optimizer', optimizer)\n",
    "print('maximum gradient', max_grad_norm)\n",
    "print('learning rate (step size)', learning_rate)\n",
    "print('epochs to train for', epoch)\n",
    "\n",
    "# create optimizer\n",
    "opt = mx.optimizer.create(optimizer)\n",
    "opt.lr = learning_rate\n",
    "\n",
    "updater = mx.optimizer.get_updater(opt)\n",
    "\n",
    "# For each training epoch\n",
    "for iteration in range(epoch):\n",
    "    tic = time.time()\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "\n",
    "    # Over each batch of training data\n",
    "    for begin in range(0, x_fast_train.shape[0], batch_size):\n",
    "        batchX = x_fast_train[begin:begin+batch_size]\n",
    "        batchY = y_fast_train[begin:begin+batch_size]\n",
    "        if batchX.shape[0] != batch_size:\n",
    "            continue\n",
    "\n",
    "        cnn_model.data[:] = batchX\n",
    "        cnn_model.label[:] = batchY\n",
    "\n",
    "        # forward\n",
    "        cnn_model.cnn_exec.forward(is_train=False)\n",
    "\n",
    "        # backward\n",
    "        cnn_model.cnn_exec.backward()\n",
    "\n",
    "        # eval on training data\n",
    "        num_correct += sum(batchY == np.argmax(cnn_model.cnn_exec.outputs[0].asnumpy(), axis=1))\n",
    "        num_total += len(batchY)\n",
    "        \n",
    "\n",
    "        # update weights\n",
    "        norm = 0\n",
    "        for idx, weight, grad, name in cnn_model.param_blocks:\n",
    "            grad /= batch_size\n",
    "            l2_norm = mx.nd.norm(grad).asscalar()\n",
    "            norm += l2_norm * l2_norm\n",
    "\n",
    "        norm = math.sqrt(norm)\n",
    "        for idx, weight, grad, name in cnn_model.param_blocks:\n",
    "            if norm > max_grad_norm:\n",
    "                grad *= (max_grad_norm / norm)\n",
    "\n",
    "            updater(idx, grad, weight)\n",
    "\n",
    "            # reset gradient to zero\n",
    "            grad[:] = 0.0\n",
    "\n",
    "    # Decay learning rate for this epoch to ensure we are not \"overshooting\" optima\n",
    "    if iteration % 50 == 0 and iteration > 0:\n",
    "        opt.lr *= 0.5\n",
    "        print('reset learning rate to %g' % opt.lr)\n",
    "\n",
    "    # End of training loop for this epoch\n",
    "    toc = time.time()\n",
    "    train_time = toc - tic\n",
    "    train_acc = num_correct * 100 / float(num_total)\n",
    "\n",
    "    # Saving checkpoint to disk\n",
    "    if (iteration + 1) % 10 == 0:\n",
    "        prefix = 'cnn'\n",
    "        cnn_model.symbol.save('./%s-symbol.json' % prefix)\n",
    "        save_dict = {('arg:%s' % k) : v  for k, v in cnn_model.cnn_exec.arg_dict.items()}\n",
    "        save_dict.update({('aux:%s' % k) : v for k, v in cnn_model.cnn_exec.aux_dict.items()})\n",
    "        param_name = './%s-%04d.params' % (prefix, iteration)\n",
    "        mx.nd.save(param_name, save_dict)\n",
    "        print('Saved checkpoint to %s' % param_name)\n",
    "\n",
    "\n",
    "    # Evaluate model after this epoch on dev (test) set\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "\n",
    "    # For each test batch\n",
    "    for begin in range(0, x_test.shape[0], batch_size):\n",
    "        batchX = x_fast_test[begin:begin+batch_size]\n",
    "        batchY = y_fast_test[begin:begin+batch_size]\n",
    "\n",
    "        if batchX.shape[0] != batch_size:\n",
    "            continue\n",
    "\n",
    "        cnn_model.data[:] = batchX\n",
    "        cnn_model.cnn_exec.forward(is_train=False)\n",
    "\n",
    "        num_correct += sum(batchY == np.argmax(cnn_model.cnn_exec.outputs[0].asnumpy(), axis=1))\n",
    "        num_total += len(batchY)\n",
    "        predicted.append(cnn_model.cnn_exec.outputs[0].asnumpy())\n",
    "        actual.append(batchY)\n",
    "        max_act.append(np.argmax(cnn_model.cnn_exec.outputs[0].asnumpy(), axis=1))\n",
    "\n",
    "    dev_acc = num_correct * 100 / float(num_total)\n",
    "    print('Iter [%d] Train: Time: %.3fs, Training Accuracy: %.3f \\\n",
    "            --- Dev Accuracy thus far: %.3f' % (iteration, train_time, train_acc, dev_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score after using fasttext: ('f1', 0.5604395604395606)\n"
     ]
    }
   ],
   "source": [
    "t=[]\n",
    "j=[]\n",
    "l=[]\n",
    "for pred in predicted:\n",
    "    for i in pred:\n",
    "        t.append(i)\n",
    "for act in actual:\n",
    "    for i in act:\n",
    "        j.append(i)\n",
    "for maxx in max_act:\n",
    "    for i in maxx:\n",
    "        l.append(i)\n",
    "eval_metrics_2 = mx.metric.F1()\n",
    "eval_metrics_2.update(labels = mx.nd.array(j), preds = mx.nd.array(t))\n",
    "print(\"f1 score after using fasttext:\",eval_metrics_2.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f1-score improved after using fasttext because of the sub-word information provided to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
